Metadata-Version: 2.4
Name: my-project
Version: 0.1.0
Summary: Project generated from pinned requirements
Author: Your Name
License-Expression: MIT
Requires-Python: <3.11,>=3.8
Description-Content-Type: text/markdown
License-File: LICENSE.md
License-File: LICENSE.pdf
Requires-Dist: absl-py==2.1.0
Requires-Dist: alembic==1.13.1
Requires-Dist: antlr4-python3-runtime==4.8
Requires-Dist: appdirs==1.4.4
Requires-Dist: cachetools==5.3.3
Requires-Dist: certifi==2021.5.30
Requires-Dist: chardet==5.2.0
Requires-Dist: charset-normalizer==3.3.2
Requires-Dist: click==8.1.7
Requires-Dist: cloudpickle<3,>=1.6
Requires-Dist: colorama==0.4.6
Requires-Dist: colorlog==6.8.2
Requires-Dist: configparser==6.0.1
Requires-Dist: cycler==0.10.0
Requires-Dist: DataProperty==1.0.1
Requires-Dist: docker-pycreds==0.4.0
Requires-Dist: Farama-Notifications==0.0.4
Requires-Dist: filelock==3.13.1
Requires-Dist: fsspec==2024.2.0
Requires-Dist: future==1.0.0
Requires-Dist: gitdb==4.0.11
Requires-Dist: GitPython==3.1.42
Requires-Dist: google-auth==2.28.1
Requires-Dist: google-auth-oauthlib==1.0.0
Requires-Dist: gql==0.2.0
Requires-Dist: graphql-core==1.1
Requires-Dist: greenlet==3.0.3
Requires-Dist: grpcio==1.62.0
Requires-Dist: gym==0.23.1
Requires-Dist: huggingface-hub==0.22.2
Requires-Dist: hydra-core==1.0.0
Requires-Dist: idna==3.6
Requires-Dist: importlib-metadata==7.0.1
Requires-Dist: importlib_resources==6.1.2
Requires-Dist: Jinja2==3.1.3
Requires-Dist: kiwisolver==1.3.1
Requires-Dist: Mako==1.3.3
Requires-Dist: Markdown==3.5.2
Requires-Dist: markdown-it-py==3.0.0
Requires-Dist: MarkupSafe==2.1.5
Requires-Dist: matplotlib==3.5.3
Requires-Dist: mbstrdecoder==1.1.3
Requires-Dist: mdurl==0.1.2
Requires-Dist: mpmath==1.3.0
Requires-Dist: networkx==3.1
Requires-Dist: numpy==1.21.6
Requires-Dist: nvidia-ml-py3==7.352.0
Requires-Dist: oauthlib==3.2.2
Requires-Dist: omegaconf==2.1.2
Requires-Dist: opencv-python==4.9.0.80
Requires-Dist: optuna==3.6.1
Requires-Dist: packaging==23.2
Requires-Dist: pandas==1.3.5
Requires-Dist: pathvalidate==3.2.0
Requires-Dist: Pillow==9.5.0
Requires-Dist: promise==2.3
Requires-Dist: protobuf==4.25.3
Requires-Dist: psutil==5.9.8
Requires-Dist: pyasn1==0.5.1
Requires-Dist: pyasn1-modules==0.3.0
Requires-Dist: pygame==2.1.0
Requires-Dist: pyglet==1.5.0
Requires-Dist: Pygments==2.17.2
Requires-Dist: pyparsing==2.4.7
Requires-Dist: pytablewriter==1.2.0
Requires-Dist: python-dateutil==2.8.2
Requires-Dist: pytz==2021.1
Requires-Dist: PyYAML==6.0.1
Requires-Dist: requests==2.31.0
Requires-Dist: requests-oauthlib==1.3.1
Requires-Dist: rich==13.7.1
Requires-Dist: rsa==4.9
Requires-Dist: scipy==1.7.3
Requires-Dist: seaborn==0.13.2
Requires-Dist: sentry-sdk==1.40.6
Requires-Dist: setproctitle==1.3.3
Requires-Dist: shortuuid==1.0.12
Requires-Dist: six==1.16.0
Requires-Dist: smmap==5.0.1
Requires-Dist: SQLAlchemy==2.0.29
Requires-Dist: stable-baselines3==1.0
Requires-Dist: subprocess32==3.5.4; python_version < "3"
Requires-Dist: sympy==1.12
Requires-Dist: tabledata==1.3.3
Requires-Dist: tcolorpy==0.1.4
Requires-Dist: tensorboard==2.14.0
Requires-Dist: tensorboard-data-server==0.7.2
Requires-Dist: tensorboardX==2.6.2.2
Requires-Dist: termcolor==2.4.0
Requires-Dist: torch==2.2.1
Requires-Dist: torchvision==0.17.1
Requires-Dist: tornado==5.1.1
Requires-Dist: tqdm==4.66.2
Requires-Dist: typepy==1.3.2
Requires-Dist: typing_extensions==4.10.0
Requires-Dist: urllib3==2.2.1
Requires-Dist: wandb==0.16.4
Requires-Dist: wasabi==1.1.2
Requires-Dist: watchdog==4.0.0
Requires-Dist: Werkzeug==3.0.1
Requires-Dist: zipp==3.17.0
Requires-Dist: pywin32==306; sys_platform == "win32"
Requires-Dist: pypiwin32==223; sys_platform == "win32"
Dynamic: license-file

# Inverse Q-Learning (IQ-Learn)

## SOTA framework for non-adversarial Imitation Learning

IQ-Learn enables very fast, scalable and stable imitation learning.
Our IQ-Learn algorithm is present in `iq.py`. This file can be used standalone to add **IQ** to your IL & RL projects. 

IQ-Learn can be implemented on top of most existing RL methods (off-policy & on-policy) by changing the critic update loss to our proposed `iq_loss`. <br>
(IQ has been successfully tested to work with Q-Learning, SAC, PPO, DDPG and Decision Transformer agents).

### Update:
 
 - Added IQ-Learn results on Humanoid-v2
 - Added support for DM Control environments
 - Released `expert_generation` script to generate your own experts from trained RL agents for new environments.

## Requirement

- pytorch (>= 1.4)
- gym
- wandb
- tensorboardX
- hydra-core=1.0 (>= 1.1 is incompatible currently)

## Installation

- Make a conda environment and install dependencies: `pip install -r requirements.txt`
- Setup wandb project to log and visualize metrics
- (Optional) Download expert datasets for Atari environments from [GDrive](https://drive.google.com/file/d/1wKdMi10_X0oV4URdkv8JSCY0rRB8iBFq/view?usp=sharing)

## Examples

We show some examples that push the boundaries of imitation learning using IQ-Learn:

### 1. CartPole-v1 using 1 demo subsampled 20 times with fully *offline* imitation  

```
python train_iq.py agent=softq method=iq env=cartpole expert.demos=1 expert.subsample_freq=20 agent.init_temp=0.001 method.chi=True method.loss=value_expert
```

IQ-Learn is the only method thats reaches the expert env reward of **500** (requiring only 3k training steps and less than 30 secs!!)

<img src="../docs/cartpole_example.png" width="500"> 

### 2. Playing Pong at human performance

```
python train_iq.py agent=softq env=pong agent.init_temp=1e-3 method.loss=value_expert method.chi=True seed=0 expert.demos=30
```

Again, IQ-Learn is the only method thats reaches the expert env reward of **21** <br>
(we find better hyperparams compared to the original paper)

<img src="../docs/pong_example.png" width="500"> 



### 3. Controlling a Humanoid with imitation of a single expert

```
python train_iq.py env=humanoid agent=sac expert.demos=1 method.loss=v0 method.regularize=True agent.actor_lr=3e-05 seed=0 agent.init_temp=1
```

IQ-Learn learns to control a full humanoid at expert performance using a single demonstration reaching the expert env reward of **5300** <br>

<img src="../docs/humanoid_example.png" width="500"> 

## Instructions
We show example code for training Q-Learning and SAC agents with **IQ-Learn** in `train_iq`.py. We make minimum modifications to original RL training code present in `train_rl`.py and simply change the critic loss function.
<!-- Our training code is present in `train_iq.py` which implements **IQ-Learn** on top of DQN/SAC RL agents by simply changing the Q-function update rule. RL training code is in `train_rl.py`. <br> IQ-Learn simplify modifies the loss function for the critic network, compared to vanilla RL. -->

- To reproduce our Offline IL experiments, see `scripts/run_offline.sh`
- To reproduce our Mujoco experiments, see `scripts/run_mujoco.sh`
- To reproduce Atari experiments, see `scripts/run_atari.sh`
- To visualize our recovered state-only rewards on a toy Point Maze environment: 
    `python -m vis.maze_vis env=pointmaze_right eval.policy=pointmaze agent.init_temp=1 agent=sac.q_net._target_=agent.sac_models.DoubleQCritic`. <br>
    Reward visualizations are saved in `vis/outputs` directory

## Contributions

Contributions are very welcome. If you know how to make this code better, please open an issue. If you want to submit a pull request, please open an issue first. 

## License

The code is made available for academic, non-commercial usage. Please see the [LICENSE](LICENSE.md) for the licensing terms of IQ-Learn for commercial use and running it on your robots/creating new AI agents.

For any inquiry, contact: Div Garg ([divgarg@stanford.edu](mailto:divgarg@stanford.edu?subject=[GitHub]%IQ-Learn))


